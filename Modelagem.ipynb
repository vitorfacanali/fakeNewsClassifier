{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SISTEMA PARA IDENTIFICAÇÃO DE NOTÍCIAS FALSAS EM IDIOMA PORTUGUÊS POR MEIO DA UTILIZAÇÃO DE TÉCNICAS DE TEXT MINING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "import spacy \n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.ticker import PercentFormatter \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score \n",
    "from sklearn.svm import SVC \n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "spell = SpellChecker(language='pt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conta_palavra(texto):\n",
    "    word_count = {}\n",
    "    words = texto.split() \n",
    "    for word in words: \n",
    "        word = word.lower() \n",
    "        if not word in word_count: \n",
    "            word_count[word] = 1 \n",
    "        else: \n",
    "            word_count[word] = word_count[word] + 1          \n",
    "\n",
    "    df_columns = pd.DataFrame([word_count])\n",
    "    df_stat = pd.DataFrame(word_count.items(), columns=['palavra', 'qtd']) \n",
    "\n",
    "    return(df_stat, df_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrigir_ortografia(texto): \n",
    "    palavras = texto.split() \n",
    "    texto_corrigido = ''\n",
    "    for i, palavra in enumerate(palavras): \n",
    "        try:\n",
    "            correcao = spell.correction(palavra)\n",
    "        except: \n",
    "            correcao = palavra \n",
    "\n",
    "        if correcao == None:\n",
    "            correcao = palavra\n",
    "        \n",
    "        if i == 0:\n",
    "            texto_corrigido = correcao\n",
    "        else:\n",
    "            texto_corrigido = texto_corrigido + ' ' + correcao \n",
    "\n",
    "    return texto_corrigido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconhecer_entidades_nomeadas(texto):\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    doc = nlp(texto)\n",
    "    entidades = [(entidade.text, entidade.label_) for entidade in doc.ents]\n",
    "\n",
    "    for entidade in entidades:\n",
    "                    texto = texto.replace(entidade[0], 'entidade_' + entidade[1]) \n",
    "\n",
    "    return texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_stopwords(texto):\n",
    "    stopwords_pt = set(stopwords.words('portuguese')) \n",
    "    texto_sem_stopwords = ''\n",
    "    texto = unidecode(texto) \n",
    "    palavras = texto.split() \n",
    "\n",
    "    for i, palavra in enumerate(palavras): \n",
    "        if palavra.lower() not in stopwords_pt:\n",
    "            if texto_sem_stopwords == '':\n",
    "                texto_sem_stopwords = palavra.lower()\n",
    "            else:\n",
    "                texto_sem_stopwords = texto_sem_stopwords + ' ' + palavra.lower() \n",
    "    return texto_sem_stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realizar_stemming(texto): \n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    palavras = texto.split()\n",
    "\n",
    "    texto_stemmed = '' \n",
    "    for i, palavra in enumerate(palavras):\n",
    "        palavra_stemming = stemmer.stem(palavra)\n",
    "        if i == 0:\n",
    "            texto_stemmed = palavra_stemming\n",
    "        else:\n",
    "            texto_stemmed = texto_stemmed + ' ' + palavra_stemming\n",
    "        \n",
    "        dict_stemming[palavra_stemming] = palavra \n",
    "\n",
    "    return texto_stemmed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_sinonimos(palavra):\n",
    "    sinonimos = set()\n",
    "    for syn in wordnet.synsets(palavra, lang=\"por\"):\n",
    "        for lemma in syn.lemmas(lang=\"por\"):\n",
    "            sinonimos.add(lemma.name())\n",
    "    \n",
    "    return list(sinonimos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulacao_sinonimos(texto, dicionario_de_sinonimos):\n",
    "    palavras = nltk.word_tokenize(texto)\n",
    "    novo_texto = []   \n",
    "\n",
    "    for palavra in palavras:\n",
    "        if palavra in dicionario_de_sinonimos: \n",
    "            sinonimos = dicionario_de_sinonimos[palavra] \n",
    "            if sinonimos: \n",
    "                novo_texto.append(sinonimos[0])# Substitui pela primeira sugestão de sinônimo\n",
    "            \n",
    "            else:\n",
    "                novo_texto.append(palavra) \n",
    "        else:\n",
    "            novo_texto.append(palavra) \n",
    "\n",
    "    return ' '.join(novo_texto) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(textos, palavras):\n",
    "    word_counts = {word: [] for word in palavras}\n",
    "\n",
    "    for texto in textos:\n",
    "        counts = {word: 0 for word in palavras} \n",
    "        text_words = texto.split() \n",
    "        for word in text_words: \n",
    "            if word in counts: \n",
    "                counts[word] += 1 \n",
    "        for word in palavras: \n",
    "            word_counts[word].append(counts[word])\n",
    "\t\t\t\n",
    "    df = pd.DataFrame(word_counts)     \n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = r'data\\full_texts' \n",
    "lista_texto_fake = []\n",
    "lista_texto_true = []\n",
    "\t\n",
    "for diretorio, subpastas, arquivos in os.walk(pasta): \n",
    "    for subpasta in subpastas:\n",
    "        if subpasta in ['fake','true']: \n",
    "            for arquivo in (os.listdir(pasta + '/' + subpasta)): \n",
    "                with open(pasta + '/' + subpasta + '/' + arquivo, encoding=\"utf8\") as f: \n",
    "                    texto_atual = f.read() \n",
    "                    if subpasta == 'fake': \n",
    "                        lista_texto_fake.append(texto_atual) \n",
    "                    else:\n",
    "                        lista_texto_true.append(texto_atual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra_fake = random.sample(lista_texto_fake, int(len(lista_texto_fake) * 0.10)) \n",
    "amostra_true = random.sample(lista_texto_true, int(len(lista_texto_true) * 0.10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_texto_amostral = amostra_fake + amostra_true\n",
    "lista_texto_amostral_trat = [] \n",
    "\n",
    "for texto in tqdm(lista_texto_amostral): \n",
    "    texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto) \n",
    "    texto_corrigido = corrigir_ortografia(texto) \n",
    "    texto_entidate = reconhecer_entidades_nomeadas(texto_corrigido) \n",
    "\n",
    "    lista_texto_amostral_trat.append(texto_entidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_pre_tratado = '' \n",
    "for texto in lista_texto_amostral_trat:\n",
    "    texto_pre_tratado = texto_pre_tratado + \" \" + texto  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição da quantidade de entradas e palavras selecionadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat, df_columns = conta_palavra(texto_pre_tratado) \n",
    "df_stat \n",
    "\n",
    "dict_stemming = {} \n",
    "\n",
    "df = df_stat.copy() \n",
    "df.index = df['palavra'].values \n",
    "df = df.sort_values(by='qtd',ascending=False) \n",
    "df[\"porcentagem_acumulada\"] = df[\"qtd\"].cumsum()/df[\"qtd\"].sum()*100 \n",
    "df['palavra_original'] = df['palavra'].map(dict_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "ax.bar(df.index, df[\"qtd\"], color=\"C0\") \n",
    "ax2 = ax.twinx() \n",
    "ax2.plot(df.index, df[\"porcentagem_acumulada\"], color=\"C1\", marker=\"D\", ms=3) \n",
    "ax2.yaxis.set_major_formatter(PercentFormatter()) \n",
    "\n",
    "ax.tick_params(axis=\"y\", colors=\"C0\") \n",
    "ax2.tick_params(axis=\"y\", colors=\"C1\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df[((df[\"porcentagem_acumulada\"]<=65) & (df[\"palavra\"].str.len() >= 3))] \n",
    "print(len(df_filtrado['palavra'].values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario_de_sinonimos = {}\n",
    "    \n",
    "for palavra_original in df_filtrado['palavra'].values: \n",
    "    dicionario_de_sinonimos[palavra_original] = obter_sinonimos(palavra_original) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré processamento total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_texto_fake_trat_total = []\n",
    "lista_texto_true_trat_total = []\n",
    "\n",
    "with open('fake_trat.txt', 'w') as f:\n",
    "    for texto in tqdm(lista_texto_fake): \n",
    "        texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "        texto_corrigido = corrigir_ortografia(texto)\n",
    "        texto_entidate = reconhecer_entidades_nomeadas(texto_corrigido)\n",
    "        texto_com_manipulacao_sinonimos = manipulacao_sinonimos(texto_entidate, dicionario_de_sinonimos)\n",
    "        texto_stop = remover_stopwords(texto_com_manipulacao_sinonimos)\n",
    "        texto_stemming = realizar_stemming(texto_stop)\n",
    "        \n",
    "        f.write(texto_stemming)\n",
    "        f.write('\\n') \n",
    "\n",
    "with open('true_trat.txt', 'w') as f: \n",
    "    for texto in tqdm(lista_texto_true):\n",
    "        texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
    "        texto_corrigido = corrigir_ortografia(texto)\n",
    "        texto_entidate = reconhecer_entidades_nomeadas(texto_corrigido)\n",
    "        texto_com_manipulacao_sinonimos = manipulacao_sinonimos(texto_entidate, dicionario_de_sinonimos)\n",
    "        texto_stop = remover_stopwords(texto_com_manipulacao_sinonimos)\n",
    "        texto_stemming = realizar_stemming(texto_stop)\t\n",
    "\n",
    "        f.write(texto_stemming)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação da tabela para modelagem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_texto_fake_trat_total = []\n",
    "lista_texto_true_trat_total = []\n",
    "\n",
    "with open('fake_trat.txt') as f:\n",
    "    lista_texto_fake_trat_total = f.readlines()\n",
    "\t\n",
    "with open('true_trat.txt') as f:\n",
    "    lista_texto_true_trat_total = f.readlines()\n",
    "\t\n",
    "lista_palavras_bagofwords = df_filtrado['palavra'].values \n",
    "\n",
    "df_fake = bag_of_words(lista_texto_fake_trat_total, lista_palavras_bagofwords)\n",
    "df_fake['target'] = 1\t\n",
    "\n",
    "df_true = bag_of_words(lista_texto_true_trat_total, lista_palavras_bagofwords)\n",
    "df_true['target'] = 0\n",
    "\n",
    "df_total = df_fake.append(df_true).reset_index(drop=True)\n",
    "df_total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação da tabela final em treino, teste e validação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, rest_df = train_test_split(df_total, test_size=0.3, random_state=42, stratify=df_total[\"target\"])\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train, y_train = train_df.drop('target', axis=1), train_df['target']\n",
    "X_val, y_val = val_df.drop('target', axis=1), val_df['target']\n",
    "X_test, y_test = test_df.drop('target', axis=1), test_df['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_2 = scaler.fit_transform(X_train)\n",
    "X_val_2 = scaler.transform(X_val)\n",
    "X_test_2 = scaler.transform(X_test)\t\n",
    "\n",
    "X_train_lstm = X_train_2.reshape(X_train_2.shape[0], 1, X_train_2.shape[1])\n",
    "X_val_lstm = X_val_2.reshape(X_val_2.shape[0], 1, X_val_2.shape[1])\n",
    "X_test_lstm = X_test_2.reshape(X_test_2.shape[0], 1, X_test_2.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Aprendizado de Máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "        },\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": [\"linear\", \"rbf\"],\n",
    "        },\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "        },\n",
    "    },\n",
    "}\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_nn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=311, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\t\n",
    "\n",
    "def create_lstm_nn():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(1, 311), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_nn = {\n",
    "    \"Dense_NN\": create_dense_nn(),\n",
    "    \"LSTM\": create_lstm_nn(),\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Model\", \"Model_Name\", \"Set\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC AUC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento, validação e avaliação de métricas para modelos de aprendizado de máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_info in models.items():\n",
    "    model = model_info[\"model\"]\n",
    "    params = model_info[\"params\"]\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(model, params, cv=3, verbose=1, n_jobs=-1, scoring='roc_auc')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Avalição do modelo no conjunto de treinamento\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_train_prob = best_model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    precision = precision_score(y_train, y_train_pred)\n",
    "    recall = recall_score(y_train, y_train_pred)\n",
    "    f1 = f1_score(y_train, y_train_pred)\n",
    "    roc_auc = roc_auc_score(y_train, y_train_prob)\n",
    "    \n",
    "    results_df = results_df.append({\"Model\": model, \"Model_Name\": model_name, \"Set\": \"Train\", \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1, \"ROC AUC\": roc_auc}, ignore_index=True)\n",
    "\n",
    "    # Avalição do modelo no conjunto de validação\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    y_val_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    results_df = results_df.append({\"Model\": model, \"Model_Name\": model_name, \"Set\": \"Validation\", \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1, \"ROC AUC\": roc_auc}, ignore_index=True)     \n",
    "\n",
    "    # Avalição do modelo no conjunto de teste\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_test_prob)\n",
    "    \n",
    "    results_df = results_df.append({\"Model\": model, \"Model_Name\": model_name, \"Set\": \"Test\", \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1, \"ROC AUC\": roc_auc}, ignore_index=True)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento, validação e avaliação de métricas para modelos de redes neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_nn.items():\n",
    "    clf = model\n",
    "    if model_name == 'LSTM':\n",
    "        X_train = X_train_lstm \n",
    "        X_val = X_val_lstm\n",
    "        X_test = X_test_lstm\n",
    "        \n",
    "    clf.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Avalição do modelo no conjunto de treinamento\n",
    "    y_train_prob = clf.predict(X_train)\n",
    "    \n",
    "    accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    precision = precision_score(y_train, y_train_pred)\n",
    "    recall = recall_score(y_train, y_train_pred)\n",
    "    f1 = f1_score(y_train, y_train_pred)\n",
    "    roc_auc = roc_auc_score(y_train, y_train_prob)\n",
    "    \n",
    "    results_df = results_df.append({\"Model\": clf, \"Model_Name\": model_name, \"Set\": \"Train\", \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1, \"ROC AUC\": roc_auc}, ignore_index=True)\n",
    "\n",
    "    # Avalição do modelo no conjunto de validação\n",
    "    y_val_prob = clf.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    results_df = results_df.append({\"Model\": clf, \"Model_Name\": model_name, \"Set\": \"Validation\", \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1, \"ROC AUC\": roc_auc}, ignore_index=True) \n",
    "     \n",
    "    # Avalição do modelo no conjunto de teste\n",
    "    y_test_prob = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "    \n",
    "    results_df = results_df.append({\"Model\": clf, \"Model_Name\": model_name, \"Set\": \"Test\", \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1, \"ROC AUC\": roc_auc}, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
